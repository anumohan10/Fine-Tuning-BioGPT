{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"349f130de0eb41f2a5cbc4d5cd5eb33d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d891233f03794d64945dbd936c9dd2fe","IPY_MODEL_ebfae78ec60646aa9126f593ebba6aa0","IPY_MODEL_32d9f0015ca1480dba6dbc11b24f9f37"],"layout":"IPY_MODEL_14097afef2f4470d9b33f27eb7a890a9"}},"d891233f03794d64945dbd936c9dd2fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_764f45a3e72d4242b139a51f872a2baf","placeholder":"‚Äã","style":"IPY_MODEL_8b86924dc44c433290fa6a7b4af72385","value":"pytorch_model.bin:‚Äá100%"}},"ebfae78ec60646aa9126f593ebba6aa0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1481383428841b69e444bce53f98531","max":1560781537,"min":0,"orientation":"horizontal","style":"IPY_MODEL_466f6edc09094cb6baddcd76cbe79a4a","value":1560781537}},"32d9f0015ca1480dba6dbc11b24f9f37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24ba3a25cac743259a9cde9ca9f01e0a","placeholder":"‚Äã","style":"IPY_MODEL_dabb99b122d84751a8b4ecdc5502ff02","value":"‚Äá1.56G/1.56G‚Äá[00:23&lt;00:00,‚Äá93.3MB/s]"}},"14097afef2f4470d9b33f27eb7a890a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"764f45a3e72d4242b139a51f872a2baf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b86924dc44c433290fa6a7b4af72385":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1481383428841b69e444bce53f98531":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"466f6edc09094cb6baddcd76cbe79a4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24ba3a25cac743259a9cde9ca9f01e0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dabb99b122d84751a8b4ecdc5502ff02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12398244,"sourceType":"datasetVersion","datasetId":7818437}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import TrainingArguments\nprint(TrainingArguments.__init__.__code__.co_varnames)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ta1SUnLg8ueK","outputId":"85abfea5-2186-4df6-fbef-31a7805e939e","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:00:27.236451Z","iopub.execute_input":"2025-07-07T11:00:27.237017Z","iopub.status.idle":"2025-07-07T11:00:27.241165Z","shell.execute_reply.started":"2025-07-07T11:00:27.236993Z","shell.execute_reply":"2025-07-07T11:00:27.240487Z"}},"outputs":[{"name":"stdout","text":"('self', 'output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'evaluation_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'dispatch_batches', 'split_batches', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules')\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"**What is PEFT?**\nPEFT (Parameter-Efficient Fine-Tuning) is a method where only a small number of parameters are trained (e.g., adapters or LoRA layers), while keeping the large pretrained model frozen.\n\nThis is especially useful when working with large language models like BioGPT on limited compute resources (like Google Colab or Kaggle).\n\nüí°**Key Reasons for Using PEFT in This Project**\n1. Memory Efficiency\nFull fine-tuning BioGPT (~347M parameters) is very memory-intensive.\n\nWith PEFT (e.g., using LoRA), you train only a few million parameters, saving a lot of VRAM and allowing training on free-tier GPUs.\n\n2. Faster Training\nSince most of the model is frozen, training is much faster and more stable.\n\nIdeal for iterative experimentation and hyperparameter tuning.\n\n3. Lower Risk of Overfitting\nYou reduce the risk of overfitting to small datasets.\n\nThe pretrained knowledge of BioGPT is retained, and only task-specific layers are adapted.\n\n4. Modularity and Reusability\nPEFT layers can be saved separately and added to other base models easily.\n\nYou can mix and match adapters, or fine-tune for other related tasks without starting from scratch.","metadata":{}},{"cell_type":"markdown","source":"> ","metadata":{}},{"cell_type":"code","source":"from transformers.training_args import TrainingArguments","metadata":{"id":"vIF7J-DH8_Dm","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:00:31.212378Z","iopub.execute_input":"2025-07-07T11:00:31.212663Z","iopub.status.idle":"2025-07-07T11:00:31.216770Z","shell.execute_reply.started":"2025-07-07T11:00:31.212641Z","shell.execute_reply":"2025-07-07T11:00:31.215934Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:00:48.699192Z","iopub.execute_input":"2025-07-07T11:00:48.699793Z","iopub.status.idle":"2025-07-07T11:00:48.703357Z","shell.execute_reply.started":"2025-07-07T11:00:48.699766Z","shell.execute_reply":"2025-07-07T11:00:48.702526Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"PCRvMPu149n1"}},{"cell_type":"code","source":"!pip install transformers==4.39.3 peft==0.10.0 accelerate==0.28.0 bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_from_disk\nimport torch\nimport os","metadata":{"id":"XuXdrbq94znw","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:09.414228Z","iopub.execute_input":"2025-07-07T11:01:09.414936Z","iopub.status.idle":"2025-07-07T11:01:09.418663Z","shell.execute_reply.started":"2025-07-07T11:01:09.414911Z","shell.execute_reply":"2025-07-07T11:01:09.417954Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"!pip uninstall -y transformers\n!pip install transformers[torch]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_from_disk\n\n# Load the datasets from the Kaggle input path\ntrain_path = \"/kaggle/input/tokenized-data/tokenized_train/tokenized_train\"\nval_path = \"/kaggle/input/tokenized-data/tokenized_val/tokenized_val\"\n\ntokenized_train = load_from_disk(train_path)\ntokenized_val = load_from_disk(val_path)\n\nprint(f\"Train size: {len(tokenized_train)}\")\nprint(f\"Validation size: {len(tokenized_val)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:15.319792Z","iopub.execute_input":"2025-07-07T11:01:15.320561Z","iopub.status.idle":"2025-07-07T11:01:15.350505Z","shell.execute_reply.started":"2025-07-07T11:01:15.320538Z","shell.execute_reply":"2025-07-07T11:01:15.349756Z"}},"outputs":[{"name":"stdout","text":"Train size: 8400\nValidation size: 1800\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"load_from_disk is used to load datasets that were previously saved to disk.\n\ntrain_path and val_path specify the locations where the tokenized training and validation datasets are stored.\n\ntokenized_train = load_from_disk(train_path) loads the preprocessed training dataset.\n\ntokenized_val = load_from_disk(val_path) loads the preprocessed validation dataset.\n\nprint(f\"Train size: {len(tokenized_train)}\") prints the number of examples in the training dataset.\n\nprint(f\"Validation size: {len(tokenized_val)}\") prints the number of examples in the validation dataset.","metadata":{}},{"cell_type":"markdown","source":"# Device configuration","metadata":{"id":"6wX_NUiq5P2_"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8v-1vyBT5M4X","outputId":"8f94bdc6-7d19-4e43-f1db-497aec0ea773","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:19.096487Z","iopub.execute_input":"2025-07-07T11:01:19.096782Z","iopub.status.idle":"2025-07-07T11:01:19.101791Z","shell.execute_reply.started":"2025-07-07T11:01:19.096758Z","shell.execute_reply":"2025-07-07T11:01:19.100961Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"torch.device(...) sets the device where your PyTorch tensors and models will be placed (either GPU or CPU).\n\n\"cuda\" if torch.cuda.is_available() else \"cpu\" checks if a GPU (CUDA) is available:\n\nIf yes, it uses the GPU (\"cuda\").\n\nIf not, it falls back to the CPU (\"cpu\").\n\ndevice stores this information for later use in your code (e.g., moving your model or data to the correct device).\n\nprint(\"Using device:\", device) simply prints which device (GPU or CPU) is being used.","metadata":{}},{"cell_type":"markdown","source":"# Load model and tokenizer","metadata":{"id":"cO2ZJ5fb5fNy"}},{"cell_type":"code","source":"!pip install sacremoses","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMl4t9cA5t3i","outputId":"f9bdaacf-04b5-4790-8be3-4aa1f711c078","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:49:26.932880Z","iopub.execute_input":"2025-07-07T10:49:26.933485Z","iopub.status.idle":"2025-07-07T10:49:30.215215Z","shell.execute_reply.started":"2025-07-07T10:49:26.933457Z","shell.execute_reply":"2025-07-07T10:49:30.214337Z"}},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model_name = \"microsoft/BioGPT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["349f130de0eb41f2a5cbc4d5cd5eb33d","d891233f03794d64945dbd936c9dd2fe","ebfae78ec60646aa9126f593ebba6aa0","32d9f0015ca1480dba6dbc11b24f9f37","14097afef2f4470d9b33f27eb7a890a9","764f45a3e72d4242b139a51f872a2baf","8b86924dc44c433290fa6a7b4af72385","a1481383428841b69e444bce53f98531","466f6edc09094cb6baddcd76cbe79a4a","24ba3a25cac743259a9cde9ca9f01e0a","dabb99b122d84751a8b4ecdc5502ff02"]},"id":"qZjh5Im35g8c","outputId":"d12b60ea-22b0-4d88-9177-71484a233002","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:25.031602Z","iopub.execute_input":"2025-07-07T11:01:25.032475Z","iopub.status.idle":"2025-07-07T11:01:26.866088Z","shell.execute_reply.started":"2025-07-07T11:01:25.032447Z","shell.execute_reply":"2025-07-07T11:01:26.865318Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Ensure tokenizer has pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.resize_token_embeddings(len(tokenizer))","metadata":{"id":"oWMI_-yi5-Kw","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:38.699414Z","iopub.execute_input":"2025-07-07T11:01:38.699900Z","iopub.status.idle":"2025-07-07T11:01:38.703734Z","shell.execute_reply.started":"2025-07-07T11:01:38.699875Z","shell.execute_reply":"2025-07-07T11:01:38.702938Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# ‚öôÔ∏è Apply LoRA (PEFT)","metadata":{"id":"jzmJ_GBW6R3I"}},{"cell_type":"markdown","source":"LoRA fine-tuning using PEFT to make training efficient. It updates only the q_proj and v_proj attention layers, while freezing most of the model. This reduces memory usage and speeds up training.","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"v_proj\"] # Added target_modules\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ENwxzie6Owd","outputId":"a2fd86fe-7d1c-4db6-a645-301a69a87cf5","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:43.216893Z","iopub.execute_input":"2025-07-07T11:01:43.217213Z","iopub.status.idle":"2025-07-07T11:01:43.286193Z","shell.execute_reply.started":"2025-07-07T11:01:43.217189Z","shell.execute_reply":"2025-07-07T11:01:43.285474Z"}},"outputs":[{"name":"stdout","text":"trainable params: 786,432 || all params: 347,549,696 || trainable%: 0.22627900672944337\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"This means:\n\nTotal model parameters: 347,549,696 ‚Üí the full size of BioGPT.\n\nTrainable parameters: 786,432 ‚Üí only this small portion is being updated.\n\nTrainable %: 0.23% ‚Üí less than 1% of the model is trained.\n\nThis shows that PEFT (LoRA) fine-tunes the model efficiently by updating a tiny subset of parameters, saving compute and memory.","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikiSiMY97k7o","outputId":"41205132-5660-43fb-9d3f-e36f82674cba","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # For causal language modeling like BioGPT\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:47.475835Z","iopub.execute_input":"2025-07-07T11:01:47.476393Z","iopub.status.idle":"2025-07-07T11:01:47.480019Z","shell.execute_reply.started":"2025-07-07T11:01:47.476366Z","shell.execute_reply":"2025-07-07T11:01:47.479300Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# üß™ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    fp16=True  # Enable if you're using GPU on Kaggle\n)","metadata":{"id":"qhQ8F8Bu8XnY","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:51.262673Z","iopub.execute_input":"2025-07-07T11:01:51.263427Z","iopub.status.idle":"2025-07-07T11:01:51.268383Z","shell.execute_reply.started":"2025-07-07T11:01:51.263397Z","shell.execute_reply":"2025-07-07T11:01:51.267467Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"output_dir=\"./results\": Saves model checkpoints here.\n\nevaluation_strategy=\"epoch\": Evaluates the model after every epoch.\n\nsave_strategy=\"epoch\": Saves checkpoints after each epoch.\n\nlogging_strategy=\"steps\": Logs progress every few steps.\n\nper_device_train_batch_size=4: Batch size of 4 during training per GPU.\n\nper_device_eval_batch_size=4: Same for evaluation.\n\nnum_train_epochs=3: Trains for 3 complete passes through the dataset.\n\nlearning_rate=5e-5: Sets the learning rate for optimization.\n\nweight_decay=0.01: Adds regularization to avoid overfitting.\n\nlogging_dir=\"./logs\": Saves logs in this directory.\n\nlogging_steps=100: Logs training loss every 100 steps.\n\nsave_total_limit=2: Keeps only the 2 most recent checkpoints.\n\nload_best_model_at_end=True: Automatically loads the best checkpoint at the end (based on validation loss).\n\nreport_to=\"none\": Disables logging to tools like TensorBoard or WandB.\n\nfp16=True: Uses mixed precision (faster, less memory) if GPU is available.\n\nüí° What's new or helpful here:\nEnables efficient GPU usage (fp16=True).\n\nAuto-loads best model to avoid manually checking validation scores.\n\nRegular evaluation/saving/logging, helpful for tracking performance.","metadata":{}},{"cell_type":"markdown","source":"# Train the model","metadata":{"id":"jIlCsYfq9tsx"}},{"cell_type":"code","source":"# üß† Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlWDjLcO9m6Z","outputId":"c056826c-23fd-48e7-cfcb-7bf1709bc871","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:01:57.014545Z","iopub.execute_input":"2025-07-07T11:01:57.015482Z","iopub.status.idle":"2025-07-07T11:01:57.035507Z","shell.execute_reply.started":"2025-07-07T11:01:57.015444Z","shell.execute_reply":"2025-07-07T11:01:57.034906Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"zONN_KL39vbd","outputId":"8b3ca253-149c-4786-f937-4913f0b60105","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:02:02.538447Z","iopub.execute_input":"2025-07-07T11:02:02.538730Z","iopub.status.idle":"2025-07-07T12:29:31.539409Z","shell.execute_reply.started":"2025-07-07T11:02:02.538705Z","shell.execute_reply":"2025-07-07T12:29:31.538635Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3150' max='3150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3150/3150 1:27:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.280400</td>\n      <td>3.144971</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.179400</td>\n      <td>3.055067</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.122400</td>\n      <td>3.032788</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3150, training_loss=3.284864235529824, metrics={'train_runtime': 5248.5615, 'train_samples_per_second': 4.801, 'train_steps_per_second': 0.6, 'total_flos': 2.34641386635264e+16, 'train_loss': 3.284864235529824, 'epoch': 3.0})"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"**What it means:**\n1. Training Loss: Measures how well the model is fitting the training data.\n\n2. Validation Loss: Measures how well the model generalizes to unseen (validation) data.\n\nüß† Interpretation:\nBoth losses decrease gradually, which is a positive sign.\n\nThe gap between training and validation loss is small, suggesting:\n\nNo overfitting yet.\n\nThe model is learning useful general patterns from the data.\n\nüìå Summary:\nThe model consistently improves with each epoch, and the learning process remains stable. You could optionally train for more epochs to check if it continues improving, but it's already showing solid convergence.","metadata":{}},{"cell_type":"code","source":"tokenizer.save_pretrained(\"finetuned_biogpt/tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T12:35:01.233012Z","iopub.execute_input":"2025-07-07T12:35:01.233565Z","iopub.status.idle":"2025-07-07T12:35:01.329258Z","shell.execute_reply.started":"2025-07-07T12:35:01.233539Z","shell.execute_reply":"2025-07-07T12:35:01.328653Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"('finetuned_biogpt/tokenizer/tokenizer_config.json',\n 'finetuned_biogpt/tokenizer/special_tokens_map.json',\n 'finetuned_biogpt/tokenizer/vocab.json',\n 'finetuned_biogpt/tokenizer/merges.txt',\n 'finetuned_biogpt/tokenizer/added_tokens.json')"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"import shutil\n\n# Create a ZIP file from the model directory\nshutil.make_archive(\"/kaggle/working/finetuned_biogpt\", 'zip', \"/kaggle/working/finetuned_biogpt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save your model and tokenizer manually to /kaggle/working\nmodel_output_path = \"/kaggle/working/finetuned_biogpt\"\nos.makedirs(model_output_path, exist_ok=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-07T15:08:45.701Z"}},"outputs":[],"execution_count":null}]}